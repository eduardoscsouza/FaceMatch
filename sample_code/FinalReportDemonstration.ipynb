{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Local Libs\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import *\n",
    "from training import *\n",
    "from data_generators import *\n",
    "from model_builders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model filepaths\n",
    "\n",
    "bbox_model_filepath = \"../experiments/results/grid_search/imgsize-224_convblocks-4_basefilters-16_densesize-1024/best_model.h5\"\n",
    "bbox_separable_model_filepath = \"../experiments/results/separable_grid_search/imgsize-112_convblocks-3_basefilters-16_densesize-1024/best_model.h5\"\n",
    "triplet_extractor_model_filepath = \"../experiments/results/triplet_base/dist-eucl/best_model.h5\"\n",
    "triplet_extractor_distances_filepath = \"../experiments/results/triplet_base/dist-eucl/distances.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function for camera demonstartion\n",
    "\n",
    "def crop_face(frame, model, img_size=(224, 224), ret_color=(255, 0, 0), upper_text=\"Camera - Press Q to quit\"):\n",
    "    if (np.prod(frame.shape) != 0):\n",
    "        aux_frame = cv2.cvtColor(cv2.resize(frame, img_size, interpolation=cv2.INTER_LINEAR), cv2.COLOR_BGR2RGB)\n",
    "        aux_frame = (aux_frame/255.0)[np.newaxis, :, :, :]\n",
    "        bbox = model.predict(aux_frame)[0]\n",
    "        bbox[[0, 2]] *= float(frame.shape[1])\n",
    "        bbox[[1, 3]] *= float(frame.shape[0])\n",
    "        bbox = np.around(bbox).astype(np.int32)\n",
    "\n",
    "        face = np.copy(frame[bbox[1]:bbox[3], bbox[0]:bbox[2]])\n",
    "        cv2.rectangle(frame, tuple(bbox[:2]), tuple(bbox[2:]), ret_color, 2)\n",
    "\n",
    "        cv2.imshow(upper_text, frame)\n",
    "        cv2.imshow('Face', face)\n",
    "    \n",
    "    return face\n",
    "\n",
    "def get_comparison_imgs(model, img_size=(224, 224)):\n",
    "    comp_imgs = []\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    print(\"Take 5 photos for comparison. Press F to take the photo\")\n",
    "    while(len(comp_imgs) < 5):\n",
    "        _, frame = cam.read()\n",
    "        face = crop_face(frame, model, img_size, upper_text=\"Press F to take the photo\")\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('f')) and (face is not None):\n",
    "            comp_imgs += [face]\n",
    "            print(\"Picture Taken\")\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"Images For Comparison\")\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(40, 20))\n",
    "    for img, ax in zip(comp_imgs, axs):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    return comp_imgs\n",
    "\n",
    "def display_camera_bbox(model, img_size=(224, 224)):\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    while(True):\n",
    "        _, frame = cam.read()\n",
    "        _ = crop_face(frame, model, img_size)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def get_only_extractor(verf_model, extract_layer_name=\"vgg16_face_extractor/\"):\n",
    "    verf_model = verf_model.get_layer(name=extract_layer_name)\n",
    "    verf_model = Model(verf_model.input, verf_model.output)\n",
    "    return verf_model\n",
    "\n",
    "def display_camera_verification(bbox_model, verf_model, comp_imgs,\n",
    "                                bbox_img_size=(224, 224), verf_img_size=(224, 224),\n",
    "                                dist_type='eucl', threshold=1.0,\n",
    "                                preprocess_func=vgg16.preprocess_input,\n",
    "                                verf_freq=25,\n",
    "                                extract_layer_name=\"vgg16_face_extractor/\"):\n",
    "    \n",
    "    verf_model = get_only_extractor(verf_model, extract_layer_name=extract_layer_name)\n",
    "    comp_imgs = [cv2.resize(img, verf_img_size, interpolation=cv2.INTER_LINEAR) for img in comp_imgs]\n",
    "    comp_imgs = np.stack([cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in comp_imgs]).astype(np.float32)\n",
    "    comp_imgs = verf_model.predict(preprocess_func(comp_imgs))\n",
    "    \n",
    "    count, same = 0, False\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    while(True):\n",
    "        _, frame = cam.read()\n",
    "        face = crop_face(frame, bbox_model, bbox_img_size, ret_color=((0, 255, 0) if same else (0, 0, 255)),\n",
    "                        upper_text=\"Green=Same Person; Red=Different Person - Press Q to quit\")\n",
    "        if face is not None:\n",
    "            if count < verf_freq:\n",
    "                count += 1\n",
    "            else:\n",
    "                vect = cv2.cvtColor(cv2.resize(face, verf_img_size, interpolation=cv2.INTER_LINEAR), cv2.COLOR_BGR2RGB)\n",
    "                vect = preprocess_func(vect.astype(np.float32))[np.newaxis, :, :, :]\n",
    "                vect = verf_model.predict(vect)[0]\n",
    "                \n",
    "                same = np.sum(np.square(comp_imgs-vect), axis=1)\n",
    "                same = np.count_nonzero(same < threshold) >= 3\n",
    "                count = 0\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples of Face Segmentation Using the Camera as Input\n",
    "\n",
    "# Regular model\n",
    "model = load_model(bbox_model_filepath, compile=False)\n",
    "display_camera_bbox(model, img_size=(224, 224))\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model using Depth-Wise Separable Convolution\n",
    "model = load_model(bbox_separable_model_filepath, compile=False)\n",
    "display_camera_bbox(model, img_size=(112, 112))\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Samples of Face Verfication Using the Camera as Input\n",
    "\n",
    "bbox_model = load_model(bbox_separable_model_filepath, compile=False)\n",
    "comp_imgs = get_comparison_imgs(bbox_model, img_size=(112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verf_thresh = pd.read_csv(triplet_extractor_distances_filepath)\n",
    "verf_thresh = verf_thresh.loc[verf_thresh[\"Set\"] == \"Train\"]\n",
    "verf_thresh = float((verf_thresh[\"Pos Dist Mean\"] + verf_thresh[\"Neg Dist Mean\"]) / 2.0)\n",
    "verf_model = load_model(triplet_extractor_model_filepath, compile=False, custom_objects={'L2Normalization':L2Normalization})\n",
    "verf_model = build_triplet_classifier_model(verf_model, dist_type='eucl', threshold=verf_thresh)\n",
    "display_camera_verification(bbox_model, verf_model, comp_imgs,\n",
    "                            bbox_img_size=(112, 112), verf_img_size=(224, 224),\n",
    "                            dist_type='eucl', threshold=verf_thresh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
